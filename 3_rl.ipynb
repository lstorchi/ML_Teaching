{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b355f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "722f8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_ROWS = 5\n",
    "GRID_COLS = 6\n",
    "ACTION_COUNT = 4  # 0: Up, 1: Down, 2: Left, 3: Right\n",
    "actionmap = {0: \"UP\", 1: \"DOWN\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "STATE_COUNT = GRID_ROWS * GRID_COLS  # This will correctly be 30\n",
    "statemap = {0: \"START\", 1: \"BLANK\", 2: \"BOMB\", 3: \"CHARGE\", 4: \"GOAL\"}\n",
    "\n",
    "# Define special states\n",
    "START_STATE = (0, 0)\n",
    "GOAL_STATE = (4, 5)\n",
    "CHARGE_STATES = [(0, 2), (4, 0), (2,2), (2,5), (4,1)]\n",
    "BOMB_STATES = [(1, 1), (1, 4), (3, 0), (3, 3)]\n",
    "\n",
    "# Define Rewards and Penalties\n",
    "REWARDS = np.full([GRID_ROWS, GRID_COLS], -1)  # -1 for each step \n",
    "REWARDS[GOAL_STATE] = 100          \n",
    "for bomb in BOMB_STATES:\n",
    "    REWARDS[bomb] = -100\n",
    "\n",
    "# define matrix for state mapping\n",
    "state_matrix = np.zeros((GRID_ROWS, GRID_COLS), dtype=int)\n",
    "for r in range(GRID_ROWS):\n",
    "    for c in range(GRID_COLS):\n",
    "        if (r, c) == START_STATE:\n",
    "            state_matrix[r, c] = 0  # START\n",
    "        elif (r, c) == GOAL_STATE:\n",
    "            state_matrix[r, c] = 4  # GOAL\n",
    "        elif (r, c) in CHARGE_STATES:\n",
    "            state_matrix[r, c] = 3  # CHARGE\n",
    "        elif (r, c) in BOMB_STATES:\n",
    "            state_matrix[r, c] = 2  # BOMB\n",
    "        else:\n",
    "            state_matrix[r, c] = 1  # BLANK\n",
    "\n",
    "LEARNING_RATE = 0.1  # (alpha) How much we update Q-values from new info\n",
    "DISCOUNT_FACTOR = 0.9  # (gamma) How much we value future rewards\n",
    "EPISODES = 1000        # How many times the agent plays the game\n",
    "\n",
    "# Exploration vs. Exploitation\n",
    "epsilon = 1.0           # 100% exploration at the start\n",
    "EPSILON_DECAY = 0.999   # How much epsilon decays after each episode\n",
    "MIN_EPSILON = 0.1       # Minimum exploration rate\n",
    "\n",
    "# Q-Table \n",
    "q_table = np.zeros((STATE_COUNT, ACTION_COUNT)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df27f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Q-Table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# print initial Q-Table\n",
    "print(\"Initial Q-Table:\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679d5313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards Table:\n",
      "[[  -1   -1   -1   -1   -1   -1]\n",
      " [  -1 -100   -1   -1 -100   -1]\n",
      " [  -1   -1   -1   -1   -1   -1]\n",
      " [-100   -1   -1 -100   -1   -1]\n",
      " [  -1   -1   -1   -1   -1  100]]\n"
     ]
    }
   ],
   "source": [
    "# print Rewards Table   \n",
    "print(\"Rewards Table:\")\n",
    "print(REWARDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d648854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_position(current_position, action):\n",
    "\n",
    "    row = current_position[0]\n",
    "    col = current_position[1]\n",
    "\n",
    "    if action == 0:  # Up\n",
    "        row = max(0, row - 1)\n",
    "    elif action == 1:  # Down\n",
    "        row = min(GRID_ROWS - 1, row + 1)\n",
    "    elif action == 2:  # Left\n",
    "        col = max(0, col - 1)\n",
    "    elif action == 3:  # Right\n",
    "        col = min(GRID_COLS - 1, col + 1)\n",
    "\n",
    "    return (row, col)\n",
    "\n",
    "def get_state(position):\n",
    "    return state_matrix[position[0], position[1]]\n",
    "\n",
    "def position_to_state_index(position):\n",
    "    \"\"\" Converts a (row, col) tuple into a unique state index (0 to 29). \"\"\"\n",
    "    return position[0] * GRID_COLS + position[1]\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    \"\"\"\n",
    "    Epsilon-Greedy Strategy:\n",
    "    - With probability (epsilon), choose a random action (Explore)\n",
    "    - With probability (1 - epsilon), choose the best action from Q-table (Exploit)\n",
    "    \"\"\"\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        # Explore: choose a random action\n",
    "        return random.randint(0, ACTION_COUNT - 1)\n",
    "    else:\n",
    "        # Exploit: choose the best action (highest Q-value) for this state\n",
    "        return np.argmax(q_table[state, :])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd53daba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agent...\n",
      "Starting Episode 0\n",
      "Starting Episode 10\n",
      "Starting Episode 20\n",
      "Starting Episode 30\n",
      "Starting Episode 40\n",
      "Starting Episode 50\n",
      "Starting Episode 60\n",
      "Starting Episode 70\n",
      "Starting Episode 80\n",
      "Starting Episode 90\n",
      "Starting Episode 100\n",
      "Starting Episode 110\n",
      "Starting Episode 120\n",
      "Starting Episode 130\n",
      "Starting Episode 140\n",
      "Starting Episode 150\n",
      "Starting Episode 160\n",
      "Starting Episode 170\n",
      "Starting Episode 180\n",
      "Starting Episode 190\n",
      "Starting Episode 200\n",
      "Starting Episode 210\n",
      "Starting Episode 220\n",
      "Starting Episode 230\n",
      "Starting Episode 240\n",
      "Starting Episode 250\n",
      "Starting Episode 260\n",
      "Starting Episode 270\n",
      "Starting Episode 280\n",
      "Starting Episode 290\n",
      "Starting Episode 300\n",
      "Starting Episode 310\n",
      "Starting Episode 320\n",
      "Starting Episode 330\n",
      "Starting Episode 340\n",
      "Starting Episode 350\n",
      "Starting Episode 360\n",
      "Starting Episode 370\n",
      "Starting Episode 380\n",
      "Starting Episode 390\n",
      "Starting Episode 400\n",
      "Starting Episode 410\n",
      "Starting Episode 420\n",
      "Starting Episode 430\n",
      "Starting Episode 440\n",
      "Starting Episode 450\n",
      "Starting Episode 460\n",
      "Starting Episode 470\n",
      "Starting Episode 480\n",
      "Starting Episode 490\n",
      "Starting Episode 500\n",
      "Starting Episode 510\n",
      "Starting Episode 520\n",
      "Starting Episode 530\n",
      "Starting Episode 540\n",
      "Starting Episode 550\n",
      "Starting Episode 560\n",
      "Starting Episode 570\n",
      "Starting Episode 580\n",
      "Starting Episode 590\n",
      "Starting Episode 600\n",
      "Starting Episode 610\n",
      "Starting Episode 620\n",
      "Starting Episode 630\n",
      "Starting Episode 640\n",
      "Starting Episode 650\n",
      "Starting Episode 660\n",
      "Starting Episode 670\n",
      "Starting Episode 680\n",
      "Starting Episode 690\n",
      "Starting Episode 700\n",
      "Starting Episode 710\n",
      "Starting Episode 720\n",
      "Starting Episode 730\n",
      "Starting Episode 740\n",
      "Starting Episode 750\n",
      "Starting Episode 760\n",
      "Starting Episode 770\n",
      "Starting Episode 780\n",
      "Starting Episode 790\n",
      "Starting Episode 800\n",
      "Starting Episode 810\n",
      "Starting Episode 820\n",
      "Starting Episode 830\n",
      "Starting Episode 840\n",
      "Starting Episode 850\n",
      "Starting Episode 860\n",
      "Starting Episode 870\n",
      "Starting Episode 880\n",
      "Starting Episode 890\n",
      "Starting Episode 900\n",
      "Starting Episode 910\n",
      "Starting Episode 920\n",
      "Starting Episode 930\n",
      "Starting Episode 940\n",
      "Starting Episode 950\n",
      "Starting Episode 960\n",
      "Starting Episode 970\n",
      "Starting Episode 980\n",
      "Starting Episode 990\n",
      "Training finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "print(\"Training the agent...\")\n",
    "for episode in range(EPISODES):\n",
    "    current_position = START_STATE  # (0, 0)\n",
    "    # Convert that position to its unique Q-table index (0-29)\n",
    "    current_state_index = position_to_state_index(current_position) \n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Starting Episode {episode}\")\n",
    "\n",
    "    howmany = 0\n",
    "    while not done:\n",
    "        action = choose_action(current_state_index, epsilon)\n",
    "        next_position = get_next_position(current_position, action)\n",
    "        reward = REWARDS[next_position] \n",
    "        \n",
    "        if next_position == GOAL_STATE or (next_position in BOMB_STATES):\n",
    "            done = True\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Episode {episode} | Pos: {current_position} | Action: {actionmap[action]} | \"\n",
    "                  f\"Next Pos: {next_position} | Reward: {reward} | Epsilon: {epsilon:.3f}\")\n",
    "            \n",
    "        # Get the old Q-value using the CURRENT STATE INDEX\n",
    "        old_q_value = q_table[current_state_index, action]\n",
    "        # Convert the new position to its unique NEXT STATE INDEX\n",
    "        next_state_index = position_to_state_index(next_position)\n",
    "        # Get the maximum Q-value for the NEXT STATE INDEX\n",
    "        max_future_q = np.max(q_table[next_state_index, :])\n",
    "        # Calculate the new Q-value\n",
    "        new_q_value = old_q_value + LEARNING_RATE * \\\n",
    "            (reward + DISCOUNT_FACTOR * max_future_q - old_q_value)\n",
    "        # Update the table using the CURRENT STATE INDEX\n",
    "        q_table[current_state_index, action] = new_q_value\n",
    "        \n",
    "        # 5. Move to the next state\n",
    "        current_position = next_position\n",
    "        current_state_index = next_state_index # Use the unique index\n",
    "        howmany += 1    \n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Episode {episode} finished in {howmany} steps.\\n\")\n",
    "    # Decay epsilon after each episode\n",
    "    epsilon = max(MIN_EPSILON, epsilon * EPSILON_DECAY)\n",
    "\n",
    "print(\"Training finished!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ddbdd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Q-Table (Rounded) ---\n",
      "Actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
      "[[  32.4   32.6   32.2   37.4]\n",
      " [  37.3 -100.    32.2   42.6]\n",
      " [  42.1   48.5   36.9   43. ]\n",
      " [  13.2   53.4   27.5    2.7]\n",
      " [  -1.5  -92.    -1.7   14.4]\n",
      " [   2.8   45.8   -1.     1.3]\n",
      " [  21.9   39.8   21.3 -100. ]\n",
      " [   0.     0.     0.     0. ]\n",
      " [  39.1   55.   -99.9   52.2]\n",
      " [  32.8   62.1   29.   -98.2]\n",
      " [   0.     0.     0.     0. ]\n",
      " [  11.7   78.2  -65.1   40. ]\n",
      " [  13.2  -99.9   10.5   47.6]\n",
      " [ -99.4   11.6   21.    54.9]\n",
      " [  46.2   39.6   46.9   62.2]\n",
      " [  53.1  -99.9   54.4   70.2]\n",
      " [ -97.2   77.    60.2   79.1]\n",
      " [  65.3   89.    69.5   76.5]\n",
      " [   0.     0.     0.     0. ]\n",
      " [  31.3   -0.7  -90.2    5.5]\n",
      " [  51.2   31.8    6.8  -86.5]\n",
      " [   0.     0.     0.     0. ]\n",
      " [  23.2   65.5  -74.6   88.9]\n",
      " [  75.1  100.    76.8   87.9]\n",
      " [ -46.9   -1.3   -1.3   -0.2]\n",
      " [  -1.6   -1.4   -1.5   11.4]\n",
      " [   5.2   10.3    0.2   50.5]\n",
      " [ -71.8   28.1   15.    72.4]\n",
      " [  38.7   51.9   14.    97.2]\n",
      " [   0.     0.     0.     0. ]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the final Q-Table (rounded for clarity)\n",
    "print(\"--- Final Q-Table (Rounded) ---\")\n",
    "print(\"Actions: 0=Up, 1=Down, 2=Left, 3=Right\")\n",
    "print(np.round(q_table, 1))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401daf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Learned Path ---\n",
      "Agent reached the GOAL!\n",
      "S → ↓ . . .\n",
      ". B ↓ . B .\n",
      ". . → → → ↓\n",
      "B . . B . ↓\n",
      "C C . . . G\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Final Learned Path ---\")\n",
    "\n",
    "# A grid to draw our path on\n",
    "# We can use your state_matrix to pre-fill it!\n",
    "display_grid = [[\".\" for _ in range(GRID_COLS)] for _ in range(GRID_ROWS)]\n",
    "\n",
    "for r in range(GRID_ROWS):\n",
    "    for c in range(GRID_COLS):\n",
    "        state_type = state_matrix[r, c] # Your 0-4 tile type\n",
    "        if state_type == 0:   # START\n",
    "            display_grid[r][c] = \"S\"\n",
    "        elif state_type == 2: # BOMB\n",
    "            display_grid[r][c] = \"B\"\n",
    "        elif state_type == 3: # CHARGE\n",
    "            display_grid[r][c] = \"C\"\n",
    "        elif state_type == 4: # GOAL\n",
    "            display_grid[r][c] = \"G\"\n",
    "        else:                 # BLANK\n",
    "            display_grid[r][c] = \".\"\n",
    "\n",
    "# Define symbols for the path\n",
    "action_symbols = ['↑', '↓', '←', '→']\n",
    "\n",
    "# --- Simulate the path ---\n",
    "current_position = START_STATE\n",
    "steps = 0\n",
    "path = [current_position] # To detect loops\n",
    "\n",
    "while current_position != GOAL_STATE and steps < (GRID_ROWS * GRID_COLS):\n",
    "    # 1. Convert position to the unique state index (0-29)\n",
    "    current_state_index = position_to_state_index(current_position)\n",
    "    \n",
    "    # 2. Get the BEST action from the Q-table (no more epsilon)\n",
    "    best_action = np.argmax(q_table[current_state_index, :])\n",
    "    \n",
    "    # 3. Mark the path on our display grid\n",
    "    # (Don't overwrite the 'S' at the start)\n",
    "    if current_position != START_STATE:\n",
    "        display_grid[current_position[0]][current_position[1]] = action_symbols[best_action]\n",
    "        \n",
    "    # 4. Move to the next position\n",
    "    current_position = get_next_position(current_position, best_action)\n",
    "    \n",
    "    # 5. Check if we hit a bomb\n",
    "    if current_position in BOMB_STATES:\n",
    "        print(\"Agent learned to run into a BOMB! ☠️\")\n",
    "        display_grid[current_position[0]][current_position[1]] = \"☠️\"\n",
    "        break\n",
    "        \n",
    "    # 6. Check if we are stuck in a loop\n",
    "    if current_position in path:\n",
    "        print(\"Agent got stuck in a loop. Training failed.\")\n",
    "        display_grid[current_position[0]][current_position[1]] = \"O\"\n",
    "        break\n",
    "    \n",
    "    path.append(current_position)\n",
    "    steps += 1\n",
    "\n",
    "if current_position == GOAL_STATE:\n",
    "    print(\"Agent reached the GOAL!\")\n",
    "\n",
    "# Print the final grid\n",
    "for row in display_grid:\n",
    "    # Join with a space for better formatting\n",
    "    print(\" \".join(row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lastpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
